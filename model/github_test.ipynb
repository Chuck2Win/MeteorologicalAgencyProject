{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "github test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyYVeMFK4Vo2",
        "outputId": "87b171bb-c74f-465b-bec1-9e8bd617234b"
      },
      "source": [
        "! pip3 install transformers\r\n",
        "! pip3 install kobert-transformers\r\n",
        "! pip3 install sentencepiece\r\n",
        "! pip3 install kss\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "import os\r\n",
        "os.chdir('./gdrive/My Drive/기상청')\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=38a0a98e6911545252d9b60b424345918e6258af0c83674e1aa563e4914fe286\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n",
            "Collecting kobert-transformers\n",
            "  Downloading https://files.pythonhosted.org/packages/f3/6d/f4e21513c1f26cacd68c144a428ccaa90dd92d85985e878976ebbaf06624/kobert_transformers-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: transformers>=2.9.1 in /usr/local/lib/python3.6/dist-packages (from kobert-transformers) (4.3.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from kobert-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (20.9)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->kobert-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->kobert-transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.9.1->kobert-transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.9.1->kobert-transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.9.1->kobert-transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.9.1->kobert-transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=2.9.1->kobert-transformers) (3.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (2020.12.5)\n",
            "Installing collected packages: kobert-transformers\n",
            "Successfully installed kobert-transformers-0.4.1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 9.0MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting kss\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/e2/43ac92280810437a552111db85a0379dfaa5ca8ccd81d27a547e9091e5d5/kss-2.5.0-py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: kss\n",
            "Successfully installed kss-2.5.0\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-bpCkZP4hbf",
        "outputId": "690d8474-fca5-4ba7-eee1-b97d790255bb"
      },
      "source": [
        "cd MeteorologicalAgencyProject "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/기상청/MeteorologicalAgencyProject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkqFh2NxHqzV"
      },
      "source": [
        "# Classifier 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCRGGQfp4i1L",
        "outputId": "e3550097-0983-443b-9d2e-7d98f37e68e0"
      },
      "source": [
        "! python3 train.py --model Augmentation --min_model Augmentation_min_model --batch_size 16"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: 100% 371k/371k [00:00<00:00, 1.07MB/s]\n",
            "Downloading: 100% 77.8k/77.8k [00:00<00:00, 455kB/s]\n",
            "Downloading: 100% 426/426 [00:00<00:00, 345kB/s]\n",
            "Downloading: 100% 369M/369M [00:12<00:00, 28.4MB/s]\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2040 > 512). Running this sequence through the model will result in indexing errors\n",
            "8202\n",
            "epoch:   9% 9/100 [43:50<7:20:59, 290.76s/it]10\n",
            "\n",
            "  Average training loss: 3.26002\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8950    0.5113    0.6508      4101\n",
            "           1     0.6580    0.9400    0.7741      4101\n",
            "\n",
            "    accuracy                         0.7257      8202\n",
            "   macro avg     0.7765    0.7257    0.7125      8202\n",
            "weighted avg     0.7765    0.7257    0.7125      8202\n",
            "\n",
            "\n",
            " Val Average training loss: 3.17775\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9025    0.5267    0.6652      1758\n",
            "           1     0.6659    0.9431    0.7806      1758\n",
            "\n",
            "    accuracy                         0.7349      3516\n",
            "   macro avg     0.7842    0.7349    0.7229      3516\n",
            "weighted avg     0.7842    0.7349    0.7229      3516\n",
            "\n",
            "\n",
            "epoch:  18% 18/100 [1:27:30<6:37:28, 290.84s/it]20\n",
            "\n",
            "  Average training loss: 1.19037\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9218    0.8135    0.8642      4101\n",
            "           1     0.8331    0.9310    0.8793      4101\n",
            "\n",
            "    accuracy                         0.8722      8202\n",
            "   macro avg     0.8774    0.8722    0.8718      8202\n",
            "weighted avg     0.8774    0.8722    0.8718      8202\n",
            "\n",
            "\n",
            " Val Average training loss: 1.20856\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9238    0.8140    0.8654      1758\n",
            "           1     0.8338    0.9329    0.8805      1758\n",
            "\n",
            "    accuracy                         0.8734      3516\n",
            "   macro avg     0.8788    0.8734    0.8730      3516\n",
            "weighted avg     0.8788    0.8734    0.8730      3516\n",
            "\n",
            "\n",
            "epoch:  27% 27/100 [2:11:10<5:53:57, 290.92s/it]30\n",
            "\n",
            "  Average training loss: 0.53263\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9484    0.9090    0.9283      4101\n",
            "           1     0.9127    0.9505    0.9312      4101\n",
            "\n",
            "    accuracy                         0.9298      8202\n",
            "   macro avg     0.9305    0.9298    0.9297      8202\n",
            "weighted avg     0.9305    0.9298    0.9297      8202\n",
            "\n",
            "\n",
            " Val Average training loss: 0.64062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9388    0.8987    0.9183      1758\n",
            "           1     0.9029    0.9414    0.9217      1758\n",
            "\n",
            "    accuracy                         0.9201      3516\n",
            "   macro avg     0.9208    0.9201    0.9200      3516\n",
            "weighted avg     0.9208    0.9201    0.9200      3516\n",
            "\n",
            "\n",
            "epoch:  39% 39/100 [3:09:20<4:55:46, 290.92s/it]40\n",
            "\n",
            "  Average training loss: 0.26713\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9653    0.9498    0.9575      4101\n",
            "           1     0.9506    0.9659    0.9582      4101\n",
            "\n",
            "    accuracy                         0.9578      8202\n",
            "   macro avg     0.9579    0.9578    0.9578      8202\n",
            "weighted avg     0.9579    0.9578    0.9578      8202\n",
            "\n",
            "\n",
            " Val Average training loss: 0.43283\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9437    0.9243    0.9339      1758\n",
            "           1     0.9259    0.9448    0.9352      1758\n",
            "\n",
            "    accuracy                         0.9346      3516\n",
            "   macro avg     0.9348    0.9346    0.9346      3516\n",
            "weighted avg     0.9348    0.9346    0.9346      3516\n",
            "\n",
            "\n",
            "epoch:  48% 48/100 [3:53:00<4:12:08, 290.93s/it]50\n",
            "\n",
            "  Average training loss: 0.15533\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9746    0.9717    0.9731      4101\n",
            "           1     0.9718    0.9746    0.9732      4101\n",
            "\n",
            "    accuracy                         0.9732      8202\n",
            "   macro avg     0.9732    0.9732    0.9732      8202\n",
            "weighted avg     0.9732    0.9732    0.9732      8202\n",
            "\n",
            "\n",
            " Val Average training loss: 0.33168\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9437    0.9346    0.9391      1758\n",
            "           1     0.9352    0.9443    0.9397      1758\n",
            "\n",
            "    accuracy                         0.9394      3516\n",
            "   macro avg     0.9395    0.9394    0.9394      3516\n",
            "weighted avg     0.9395    0.9394    0.9394      3516\n",
            "\n",
            "\n",
            "epoch:  57% 57/100 [4:36:40<3:28:31, 290.96s/it]60\n",
            "\n",
            "  Average training loss: 0.09203\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9822    0.9798    0.9810      4101\n",
            "           1     0.9798    0.9822    0.9810      4101\n",
            "\n",
            "    accuracy                         0.9810      8202\n",
            "   macro avg     0.9810    0.9810    0.9810      8202\n",
            "weighted avg     0.9810    0.9810    0.9810      8202\n",
            "\n",
            "\n",
            " Val Average training loss: 0.28817\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9383    0.9425    0.9404      1758\n",
            "           1     0.9423    0.9380    0.9401      1758\n",
            "\n",
            "    accuracy                         0.9403      3516\n",
            "   macro avg     0.9403    0.9403    0.9403      3516\n",
            "weighted avg     0.9403    0.9403    0.9403      3516\n",
            "\n",
            "\n",
            "epoch:  63% 63/100 [5:05:40<2:59:22, 290.89s/it]early stop\n",
            "min_epoch : 60\n",
            "epoch:  63% 63/100 [5:15:09<3:05:05, 300.16s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdTRmz0tHuom"
      },
      "source": [
        "## Classifier3 train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVuDxUlqLaDL",
        "outputId": "e3778d01-b755-4989-ae97-f5eaf8004576"
      },
      "source": [
        "# train\r\n",
        "! python3 inference.py --data_file ./data/augmentation_train_data --min_model ./Augmentation_min_model --predict False"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "\n",
            " Train Average Loss: 0.00510\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9873    0.9849    0.9861      4101\n",
            "           1     0.9849    0.9873    0.9861      4101\n",
            "\n",
            "    accuracy                         0.9861      8202\n",
            "   macro avg     0.9861    0.9861    0.9861      8202\n",
            "weighted avg     0.9861    0.9861    0.9861      8202\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x84O_j1HzEy"
      },
      "source": [
        "## Classifier3 val data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Slo7K7v6PIZp",
        "outputId": "03bf28e1-feef-4355-869e-4e56dfef20cb"
      },
      "source": [
        "# val\r\n",
        "! python3 inference.py --data_file ./data/augmentation_val_data --min_model ./Augmentation_min_model --predict False"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Train Average Loss: 0.01803\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9383    0.9425    0.9404      1758\n",
            "           1     0.9423    0.9380    0.9401      1758\n",
            "\n",
            "    accuracy                         0.9403      3516\n",
            "   macro avg     0.9403    0.9403    0.9403      3516\n",
            "weighted avg     0.9403    0.9403    0.9403      3516\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj2VpM4vH3c9"
      },
      "source": [
        "## Classifier3 test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZUuNuoVPLDI",
        "outputId": "4fa46d11-d945-41e4-a0ec-55e648ba7b49"
      },
      "source": [
        "# test\r\n",
        "! python3 inference.py --data_file ./data/test_data --min_model ./Augmentation_min_model --predict False"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Train Average Loss: 0.03549\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9309    0.9442    0.9375       842\n",
            "           1     0.6781    0.6266    0.6513       158\n",
            "\n",
            "    accuracy                         0.8940      1000\n",
            "   macro avg     0.8045    0.7854    0.7944      1000\n",
            "weighted avg     0.8910    0.8940    0.8923      1000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSSM8DVnH6a9"
      },
      "source": [
        "# Classifier 1 train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VI1iZUQQRut",
        "outputId": "5f78f9d4-f400-4237-9aa3-1b7be6fafa9b"
      },
      "source": [
        "# 일반으로 진행\r\n",
        "! python3 train.py --model None --min_model non_min_model --batch_size 16"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:   8% 8/100 [26:30<5:02:03, 196.99s/it]10\n",
            "\n",
            "  Average training loss: 1.26169\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8595    0.9677    0.9104      5020\n",
            "           1     0.5318    0.1881    0.2779       978\n",
            "\n",
            "    accuracy                         0.8406      5998\n",
            "   macro avg     0.6957    0.5779    0.5942      5998\n",
            "weighted avg     0.8061    0.8406    0.8073      5998\n",
            "\n",
            "\n",
            " Val Average training loss: 1.20748\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8617    0.9726    0.9138       839\n",
            "           1     0.5660    0.1863    0.2804       161\n",
            "\n",
            "    accuracy                         0.8460      1000\n",
            "   macro avg     0.7139    0.5795    0.5971      1000\n",
            "weighted avg     0.8141    0.8460    0.8118      1000\n",
            "\n",
            "\n",
            "epoch:  16% 16/100 [52:50<4:35:51, 197.04s/it]20\n",
            "\n",
            "  Average training loss: 0.57094\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9108    0.9643    0.9368      5020\n",
            "           1     0.7379    0.5153    0.6069       978\n",
            "\n",
            "    accuracy                         0.8911      5998\n",
            "   macro avg     0.8244    0.7398    0.7718      5998\n",
            "weighted avg     0.8826    0.8911    0.8830      5998\n",
            "\n",
            "\n",
            " Val Average training loss: 0.66146\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8964    0.9595    0.9269       839\n",
            "           1     0.6667    0.4224    0.5171       161\n",
            "\n",
            "    accuracy                         0.8730      1000\n",
            "   macro avg     0.7816    0.6909    0.7220      1000\n",
            "weighted avg     0.8594    0.8730    0.8609      1000\n",
            "\n",
            "\n",
            "epoch:  28% 28/100 [1:32:10<3:56:34, 197.15s/it]30\n",
            "\n",
            "  Average training loss: 0.34839\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9444    0.9649    0.9546      5020\n",
            "           1     0.7975    0.7086    0.7504       978\n",
            "\n",
            "    accuracy                         0.9231      5998\n",
            "   macro avg     0.8710    0.8368    0.8525      5998\n",
            "weighted avg     0.9205    0.9231    0.9213      5998\n",
            "\n",
            "\n",
            " Val Average training loss: 0.54287\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9135    0.9571    0.9348       839\n",
            "           1     0.7025    0.5280    0.6028       161\n",
            "\n",
            "    accuracy                         0.8880      1000\n",
            "   macro avg     0.8080    0.7425    0.7688      1000\n",
            "weighted avg     0.8796    0.8880    0.8814      1000\n",
            "\n",
            "\n",
            "epoch:  36% 36/100 [1:58:30<3:30:18, 197.17s/it]40\n",
            "\n",
            "  Average training loss: 0.22653\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9650    0.9777    0.9713      5020\n",
            "           1     0.8772    0.8180    0.8466       978\n",
            "\n",
            "    accuracy                         0.9517      5998\n",
            "   macro avg     0.9211    0.8978    0.9089      5998\n",
            "weighted avg     0.9507    0.9517    0.9510      5998\n",
            "\n",
            "\n",
            " Val Average training loss: 0.50109\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9227    0.9535    0.9379       839\n",
            "           1     0.7068    0.5839    0.6395       161\n",
            "\n",
            "    accuracy                         0.8940      1000\n",
            "   macro avg     0.8147    0.7687    0.7887      1000\n",
            "weighted avg     0.8880    0.8940    0.8898      1000\n",
            "\n",
            "\n",
            "epoch:  44% 44/100 [2:24:50<3:04:02, 197.18s/it]early stop\n",
            "min_epoch : 40\n",
            "epoch:  44% 44/100 [2:27:52<3:08:12, 201.65s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg-bWLPWH8hU"
      },
      "source": [
        "# Classifier1 train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n70XFU4pW4Sf",
        "outputId": "42368c6f-f62c-452f-e730-2624bbee2b00"
      },
      "source": [
        "# train\r\n",
        "! python3 inference.py --data_file ./data/train_data --min_model ./non_min_model --predict False"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Train Average Loss: 0.01295\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9593    0.9873    0.9731      5020\n",
            "           1     0.9231    0.7853    0.8486       978\n",
            "\n",
            "    accuracy                         0.9543      5998\n",
            "   macro avg     0.9412    0.8863    0.9109      5998\n",
            "weighted avg     0.9534    0.9543    0.9528      5998\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl4c69V3IBSd"
      },
      "source": [
        "# Classifier1 val data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4xMrrWeW6Ce",
        "outputId": "cd3ed1d1-0dfa-4ca1-85a0-7571c535c75e"
      },
      "source": [
        "# val\r\n",
        "! python3 inference.py --data_file ./data/val_data --min_model ./non_min_model --predict False"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Train Average Loss: 0.03157\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9227    0.9535    0.9379       839\n",
            "           1     0.7068    0.5839    0.6395       161\n",
            "\n",
            "    accuracy                         0.8940      1000\n",
            "   macro avg     0.8147    0.7687    0.7887      1000\n",
            "weighted avg     0.8880    0.8940    0.8898      1000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKPksqkcIEsU"
      },
      "source": [
        "# Classifier1 test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo6ekWkbW7PR",
        "outputId": "6cba49cf-569a-4c01-c100-ffe90b252d74"
      },
      "source": [
        "# test\r\n",
        "! python3 inference.py --data_file ./data/test_data --min_model ./non_min_model --predict False"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Train Average Loss: 0.03183\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9185    0.9501    0.9340       842\n",
            "           1     0.6744    0.5506    0.6063       158\n",
            "\n",
            "    accuracy                         0.8870      1000\n",
            "   macro avg     0.7965    0.7504    0.7702      1000\n",
            "weighted avg     0.8799    0.8870    0.8822      1000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nQNDwf1ILdy"
      },
      "source": [
        "# Classifier 2 Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR781wxMxkTw",
        "outputId": "9c8c0667-ed4a-4d42-b311-3167ae8edad8"
      },
      "source": [
        "! python3 train.py --model WeightedRandomSample --min_model WeightedRandomSample_min_model --batch_size 16"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: 100% 371k/371k [00:00<00:00, 1.06MB/s]\n",
            "Downloading: 100% 77.8k/77.8k [00:00<00:00, 454kB/s]\n",
            "Downloading: 100% 426/426 [00:00<00:00, 391kB/s]\n",
            "Downloading: 100% 369M/369M [00:09<00:00, 37.4MB/s]\n",
            "epoch:   8% 8/100 [26:20<5:00:16, 195.83s/it]10\n",
            "\n",
            "  Average training loss: 2.02149\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8524    0.5534    0.6711      3005\n",
            "           1     0.6684    0.9038    0.7685      2993\n",
            "\n",
            "    accuracy                         0.7282      5998\n",
            "   macro avg     0.7604    0.7286    0.7198      5998\n",
            "weighted avg     0.7606    0.7282    0.7197      5998\n",
            "\n",
            "\n",
            " Val Average training loss: 1.82409\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8885    0.5080    0.6464       502\n",
            "           1     0.6536    0.9357    0.7696       498\n",
            "\n",
            "    accuracy                         0.7210      1000\n",
            "   macro avg     0.7710    0.7219    0.7080      1000\n",
            "weighted avg     0.7715    0.7210    0.7078      1000\n",
            "\n",
            "\n",
            "epoch:  16% 16/100 [52:30<4:34:13, 195.88s/it]20\n",
            "\n",
            "  Average training loss: 0.68517\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8999    0.8147    0.8552      2946\n",
            "           1     0.8361    0.9125    0.8726      3052\n",
            "\n",
            "    accuracy                         0.8645      5998\n",
            "   macro avg     0.8680    0.8636    0.8639      5998\n",
            "weighted avg     0.8674    0.8645    0.8640      5998\n",
            "\n",
            "\n",
            " Val Average training loss: 1.16769\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9220    0.6874    0.7876       499\n",
            "           1     0.7516    0.9421    0.8361       501\n",
            "\n",
            "    accuracy                         0.8150      1000\n",
            "   macro avg     0.8368    0.8147    0.8119      1000\n",
            "weighted avg     0.8366    0.8150    0.8119      1000\n",
            "\n",
            "\n",
            "epoch:  28% 28/100 [1:31:40<3:55:03, 195.88s/it]30\n",
            "\n",
            "  Average training loss: 0.42836\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9527    0.8992    0.9252      3046\n",
            "           1     0.9017    0.9539    0.9271      2952\n",
            "\n",
            "    accuracy                         0.9261      5998\n",
            "   macro avg     0.9272    0.9266    0.9261      5998\n",
            "weighted avg     0.9276    0.9261    0.9261      5998\n",
            "\n",
            "\n",
            " Val Average training loss: 0.79138\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9265    0.7255    0.8138       521\n",
            "           1     0.7584    0.9374    0.8385       479\n",
            "\n",
            "    accuracy                         0.8270      1000\n",
            "   macro avg     0.8425    0.8314    0.8261      1000\n",
            "weighted avg     0.8460    0.8270    0.8256      1000\n",
            "\n",
            "\n",
            "early stop\n",
            "min_epoch : 26\n",
            "epoch:  28% 28/100 [1:41:13<4:20:17, 216.91s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixoXJIS5IOp-"
      },
      "source": [
        "# Classifier 2 Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_6mFQP1xpRn",
        "outputId": "9bee424e-ad83-4007-9936-7dab844bcd99"
      },
      "source": [
        "# test\r\n",
        "! python3 inference.py --data_file ./data/test_data --min_model ./WeightedRandomSample_min_model --predict False"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Average Loss: 0.07741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9766    0.6948    0.8119       842\n",
            "           1     0.3591    0.9114    0.5152       158\n",
            "\n",
            "    accuracy                         0.7290      1000\n",
            "   macro avg     0.6679    0.8031    0.6636      1000\n",
            "weighted avg     0.8791    0.7290    0.7651      1000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}