

# Solely Transformer Based Variational Auto Encoder for Language Modeling

# Abstract

The variational auto encoder is efficient generative model and also effective in representation learning. But when using this model in natural language processing there are many difficulties because of posterior collapse also known as kl vanishing. To reduce this problem with enhancing parallelization of processing data, we recommend variational auto encoder with solely Transformer model  in natural language processing. In previous works, models were structured with not Transformer form but RNN or Transformer and RNN mixture form. We implemented our experiments with several techniques to reduce posterior collapse.  In the experiments, the results show that the Transformer model  is better than RNN based model in reconstruction and representation learning.  

# Introduction

Variational autoencoders (VAEs) [1] have been applied in many NLP tasks, including language modeling [2], semi-supervised text classification [3]. Most prominent component of a VAE is latent representation which contains many holistic features in text such as style, topic and semantic features. With this latent representation, samples from the prior latent distribution can generate diverse and exquisite sentences [2]. Due to auto regressive nature of text, an auto regressive decoder such as RNN, LSTM[4] is widely used in VAE. But using this auto regressive decoder, ignoring the latent variable in decoder occurs, so called posterior collapse[2,5]. The posterior collapse makes latent variable useless. To reduce this problem, there are many techniques such as update inference networks and generative networks in imbalanced ways[5], re-factorizing loss function[6], changing loss function[7], kl annealing[2,8]. 

 Previous VAE language modelings use recognition model and generative model in RNN ways[2], or RNN and Transformer fusion ways [9,10]. There was no approaches which use solely Transformer ways to build VAE language modelings. Transformer have many benefits prior to RNN such as vanishing gradient issue, good at sequential operations and parallelization than RNN[11]. So building language model in VAE with solely Transformer will be applied to many other fields. 

This paper makes the following contributions:

(1) We provide a novel solely transformer based variational auto encoder in language modeling. 

(2) We experiment model in several kl annealing techniques, and also with different length of sentences. With this results, We assure the model is better than previous RNN model in reconstruction and representation learning.

# Background

1) VAE

 The variational auto encoder is a deep generative model using latent variables as shown in fig(1) [1].  VAE model assume that data have latent variables which can represent data. VAE is related to Maximum Likelihood Estimation. 

Suppose that observed data point **x** is a high dimensional random vector, and latent variable **z** is in a relatively small dimensional space.  Log likelihood of the data can be expressed into :
$$
\begin{equation}
\log p_\theta(x) = \log\int_zp_\theta(x,z)dz \geq \int_z\log p_\theta(x,z)dz= \int_zq_\phi(z|x)\log \frac{p_\theta(x,z)}{q_\phi(z|x)}dz=E_{z~q_{\phi(z|x)}}[\log p_\theta(x,z)-\log q_\phi(z|x)] = ELBO
\label{eq:ELBO}
\tag{1}
\end{equation}
$$
ELBO(evidence lover bound) is a lower bound on the log-likelihood of the data. $\phi$ and $\theta$ are parameters of inference model and generative model.  In eq. (1), maximizing ELBO is equivalent of maximizing likelihood function.

We can express eq. (1) into reconstruction term and KL divergence term:
$$
\begin{equation}
\int_zq_\phi(z|x)\log \frac{p_\theta(x,z)}{q_\phi(z|x)}dz = \int_zq_\phi(z|x)\log(\frac{ p_{\theta}(x|z)p(z)}{q_\phi(z|x)})dz = E_{z~q_{\phi}(z|x)}[\log(p_{\theta}(x|z))]-KL(q_\phi(z|x)||p(z)) 
\tag{2}
\end{equation}
$$
Typically, $q_\phi(z|x)$ is diagonal Gaussian $N_k(\mu, \sigma^2I)$ ,  $p(z)$ is Gaussian $N_k(0,I)$ where $k$ is dimension of latent variable $z$ and re-parameterization trick is used. We can express each distributions as neural networks and $\phi$ and $\theta$ are parameters of neural networks [1]. With this, eq.(2) can be expressed into:
$$
\begin{equation}
E_{z~q_{\phi}(z|x)}[\log(p_{\theta}(z|x))]-KL(q_\phi(z|x)||p(z)) = E_{z~q_{\phi}(z|x)}[\log(p_{\theta}(z|x))]+\frac{1}{2}\log(|\sigma^{2}I|)|+\frac{1}{2}k-\frac{1}{2}Tr(\sigma^{2}I)-\frac{1}{2}\mu^T\mu
\tag{3}
\end{equation}
$$

![fig.1](E:\paper_mine\fig.1.jpg)

<left> Fig.1. Variational auto encoder </left>



2) VAE for sentence generation

To generate sentence which length is T, language models generate each token conditioned on the previous tokens:
$$
\begin{equation}
p(x)=\prod_{t=1}^{T}p(x_t|x_{<t})
\end{equation}
\tag{4}
$$
In variational autoencoder for sentences, both the encoder and the decoder are using auto regressive model such as LSTM[4], GRU[12].  The decoder is conditioned on the latent variables, which is designed to capture global features like style, topic, and high-level syntactic features as depicted in fig (2) and eq.(5).  

![fig2](E:\paper_mine\fig2.jpg)

<left> Fig. 2 Illustrations of variational auto encoder for sentences </left> 


$$
\begin{equation}
E_{z~q_{\phi}(z|x)}[\log(p_{\theta}(x|z))]-KL(q_\phi(z|x)||p(z)) = E_{z~q_{\phi}(z|x)}[\sum_{t=1}^{T}\log(p_{\theta}(x_{t}|z,x_{<t}))]-KL(q_\phi(z|x)||p(z))  
\tag{5}\end{equation}
$$
But when using decoder as auto regressive model, the variational auto encoder is falling into local optima, called posterior collapse or KL vanishing[2, 5]. This is because, at early step in maximizing ELBO, the latent variables don't contain meaningful information about input sequences. Then reconstruction is done only with previous tokens $x_{<t}$.

 To reduce this phenomenon, there are many techniques such as update inference networks and generative networks in imbalanced ways[5], re-factorizing loss function[6], changing loss function[7], kl annealing[2,8]. We will use kl annealing techniques for our experiment [2,8]. 

3) Transformer 

 Transformer is encoder decoder based mechanism which remove recurrence and use self-attention [11]. The Transformer allows for significantly more parallelization and have reached state of art in translation quality. Also Transformer model is used in various pretrained language models such as BERT[13], GPT[14] which have accomplished state of art in various tasks in NLP.  

![Transformer original](E:\paper_mine\Transformer original.png)

<left> Fig. 3. Illustration of Transformer architecture </left>


 Transformer is sequence to sequence structure as shown in fig.3. A encoder is composed of two sub layers, multi head self attention layer and feed forward layer. The Encoder encodes source sentence ($x_1$,$x_2$, , ... , $x_n$) into ($h_1$,$h_2$, , ... , $h_n$). 

A decoder  is composed of three sub layers, masked multi head self attention layer, encoder-decoder attention layer and feed forward layer. The masked multi head self attention layer is the multi head self attention with masks. It is for preventing model attending next tokens ($y_{j>t}$), when dealing with $y_{j<=t}$. And the encoder-decoder attention layer is calculating attention score with encoder and decoder, where query is output of masked multi head self attention layer and key, value are output of encoder.

Self Attention and multi head self attention are key of Transformer. Attention is function that maps a query with key and value. An output is weighted sum of value, where weight is similarity of  query and key as expressed in eq. (6) and shown in fig.(4). In Transformer, the similarity function of attention is scaled dot product divided with $\sqrt{d_{k}}$  where $d_{k}$ is dimension of the key.  Self attention is where the query, key, and value are from same source.   

![fig3](E:\paper_mine\fig3.png)

<left>Fig. 4. Illustrations of the attention</left>

$$
\begin{equation}
Attention(Q,K,V) = Softmax(QK^{T}/\sqrt{d_k})V
\tag{6}
\end{equation}
$$
In addition, the Transformer also uses a multi head attention. Multi head attention is a mechanism which projects query, key, value into different spaces and apply attention function as shown in fig.(5) and eq.(7). With multi head attention, we can see query, key, value in various point of view.

![fig.4](E:\paper_mine\fig.5.png)

<left>Fig. 5. Illustrations of the multi head attention</left>

$$
\begin{equation}
Multi\ Head(Q,K,V) = Concat(head_{(1)},...,head_{(h)})W^o\\
where\ head_i = Attention(QW_i^Q,KW_i^K,VW_i^V) 
\tag{7}
\end{equation}
$$

The matrices $W_i^Q \ \in \mathbb{R}^{d_{model}\times d_{q}}, W_i^K \ \in \mathbb{R}^{d_{model}\times d_{k}}, W_i^V \ \in \mathbb{R}^{d_{model}\times d_{v}}$ and $W^o \in \mathbb{R}^{hd_v \times d_{model}}$ are for linear projections. 

# Model

1. Variational auto encoder for sentence generation

We adapt the variational auto encoder to sentence generation by using Transformer encoder and decoder for both the encoder and the decoder. The encoder is same as Transformer encoder[11]. But we revise the Transformer decoder, because of discrepancy between train and inference. We want decoder to generate sentences respect to samples from latent variable $z$, in order words, there is no encoder in inference time. In variational auto encoder, encoder output to calculate encoder-decoder attention which is sub layer of the decoder in Transformer does not exist.

 To compensate this, we add latent - decoder attention layer after feed forward network as shown in fig.6. 

The training process of the model has 5 steps.

- First, The encoder encodes the embeddings of the input sequences $X$ = $[<CLS>, x_1, x_2, ..., x_n]$ where  output of $<CLS>$ represents summary of the input sequences like in BERT [13].
- Second, converts output of $<CLS>$ in encoder into mean $\mu$, variance $\sigma$  of latent variable $z$. A latent variable $z$ is sampled from Gaussian distribution, where mean is $\mu$ and variance is $\sigma^2$.  Because of the indifferentiable sampling process, we must use 'reparameterization trick' which outsources the sampling process to the additional variable $e$ whose distribution is $N(0,I)$ [1].   With mean $\mu$, and variance $\sigma^2$, the latent variable $z$ is $\mu+\sigma\otimes e$ where $\otimes$ is element wise product.
- Third, the decoder encodes the embeddings of the input sequences $Y$ = [$<BOS>,x_1,x_2,...,x_n$] where $<BOS>$ is a token of beginning of the sentence.
- Fourth, duplicates the samples of latent variable $z$ and add them with the output of the decoder. This layer we named latent-decoder attention layer, acts like encoder - decoder attention layer in original Transformer decoder.
- Last, predict the token $x_t$ based on $x_{<t}$ and $z$. The entire output sequences is Z = [$x_1,x_2,...,x_n,<EOS>$] where $<EOS>$ is a token of end of the sentence.

At inference process of the model, generates samples from prior distribution $N(0,I)$ and predicts the token the token $x_t$ with $x_{<t}$. So there is no encoder in this inference step.

![fig.6](E:\paper_mine\fig.6.jpg)

<left> fig. 6. Illustration of Transformer based varational auto encoder. O is the output of the encoder layer, the subscipts are corresponded with the input sentences.</left>



![fig7](E:\paper_mine\fig7.jpg)

<left> fig. 7. Illustration of LSTM based varational auto encoder. O is the output of the encoder layer, the subscipts are corresponded with the input sentences.</left>

# Experiments

## Setup			

1. Baseline Approach for Language Modeling

   In this work, we consider two models as our baseline. (1) LSTM variational auto encoder[2] is variational auto encoder where the encoder and decoder both are LSTM as shown in fig (7). (2) LSTM Language modeling is language modeling model where predict the next tokens based on previous tokens. The parameter settings are shown in Table (1).

2. Datasets

   We use Penn Tree Bank dataset[15] for language modeling. PTB is a dataset for language modeling and pos tagging in NLP which are selected from Wall Street Journal. And also we use news data which are related to scorching heat damage in Korea. About 130 thousand news are collected and labeled 8,048 news as 0,1. 0 is non related to disaster(such as advertisement) and 1 is related to disaster. The data can be accessed in our github[^1].

   [1^]:https://github.com/Chuck2Win/MeteorologicalAgencyProject	"(github는 아직 수정중에 있습니다(현재 비공개 상태임) 교수님 )"

3. Evaluation

   To evaluate Language Modeling, we use intrinsic evaluation and extrinsic evaluation. The intrinsic evaluation consider three aspects of the generated text. The first one is reconstruction, that how well the generated sentences are reconstructed compared with input sentences.  Reconstruction can be represented as cross entropy.

    The second one is diversity of the generated sentences. We use distinct metrics used in previous works to measure diversity [16]. The distinct metrics is calculated by the percentage of distinct unigrams or bigrams.(denoted as dist-1 and dist-2 respectively)

    And the last one is how well representation learning is done. We can use KL divergence term as index of representation learning [2]. It means if we have low value of reconstruction error and high value of KL divergence, our latent variables of variational auto encoder represent data well which means the latent variables reflects topic or style of text.  To specify how latent variables represent data, we can also use mutual information.  Mutual information, $I_{q}$ can be computed by [17]:

   
   $$
      \begin{equation}
      I_{q} = \int_x\int_zq_{\phi}(x,z)\log (q_{\phi}(x,z)/p_{d}(x)q_{\phi}(z))dzdx = \int_xp_{d}(x)\int_zq_{\phi}(z|x)\log (q_{\phi}(z|x)/q_{\phi}(z))dzdx\\
      =  \int_xp_{d}(x)\int_zq_{\phi}(z|x)\log (q_{\phi}(z|x)/p(z))dzdx + \int_xp_{d}(x)\int_zq_{\phi}(z|x)\log (p(z)/q_{\phi}(z))dzdx
      \tag{8}
      \end{equation}
   $$

      At the second term in eq.(8), It can be factorized as:
   $$
      \begin{equation}
      \int_xp_{d}(x)\int_zq_{\phi}(z|x)log(p(z)/q_{\phi}(z))dzdx = \int_z\int_xq_{\phi}(z,x)log(p(z)/q_{\phi}(z))dxdz\\ = \int_zq_{\phi}(z)log(p(z)/q_{\phi}(z))dz = -KL(q_{\phi}(z)||p(z))
      \tag{9}
      \end{equation}
   $$
      As a result, mutual information can be expressed as :

   $$
   \begin{equation}
      I_{q}=E_{x∼p_{d}(x)}[KL(q_{\phi}(z|x)||p(z))]-KL(q_{\phi}(z)||p(z))
      \tag{10}
      \end{equation}
   $$
      Where $p_{d}(x)$ is the empirical distribution and $q_{\phi}(z)$ is the marginal likelihood($q_{\phi}(z)=\sum_{n=1}^{N}q_{\phi}(x^{(n)},z)$ is approximated by Monte Carlo estimation.

      We can also represent mutual information as different way.  
   $$
      \begin{equation}I_{q} = \int_x\int_zq_{\phi}(x,z)\log (q_{\phi}(x,z)/p_{d}(x)q_{\phi}(z))dzdx = \int_xp_{d}(x)\int_zq_{\phi}(z|x)\log (q_{\phi}(z|x)/q_{\phi}(z))dzdx\\=\frac{1}{N} \sum_{n=1}^{N}\int_zq_{\phi}(z|x^{(n)})\log (q_{\phi}(z|x^{(n)})/q_{\phi}(z))dz = \frac{1}{N} \sum_{n=1}^{N}E_{z~q_{\phi}(z|x^{(n)})}[\log(q_{\phi}(z|x^{(n)})-\log(q_{\phi}(z))] 
      \tag{11}\end{equation}
   $$

      We can use Monte Carlo estimation and  simplify equation by using assumption that inference network is diagonal Gaussian N(**$\mu$, $\sigma^2$**I) where dimension is k.
   $$
      \begin{equation}
      \frac{1}{N} \sum_{n=1}^{N}E_{z~q_{\phi}(z|x^{(n)})}[\log(q_{\phi}(z|x^{(n)})-\log(q_{\phi}(z))] = \frac{1}{N} \sum_{n=1}^{N}\frac{1}{S} \sum_{s=1}^{S}[\log(q_{\phi}(z^{(s)}|x^{(n)})-\log(q_{\phi}(z^{(s)}))]\\= \frac{1}{N} \sum_{n=1}^{N}\frac{1}{S} \sum_{s=1}^{S}[-\frac{k}{2}\log(2\pi)-\sum_{j=1}^{k}\log(\sigma^{(n)}_{j})-\frac{1}{2}(z^{(s)}-\mu^{(n)})^{T}(\sigma^{2(n)}I)^{-1}(z^{(s)}-\mu^{(n)})-\log(q_{\phi}(z^{(s)}))]
      \tag{12}
      \end{equation}
   $$
      The last term in eq.(2), it can be factorized as:
   $$
      \begin{equation}
      \log(q_{\phi}(z^{(s)})) = \log(\sum_{n=1}^{N} q_{\phi}(z^{(s)},x^{(n)})) = \log( \frac{1}{N}\sum_{n=1}^{N} q_{\phi}(z^{(s)}|x^{(n)})) \\
      = -\frac{k}{2}\log(2\pi)+\log(\sum_{n=1}^{N} \frac{1}{|\sigma^{2(n)}I|^{\frac{1}{2}}}\exp(-\frac{1}{2}(z^{(s)}-\mu^{(n)})^{T}(\sigma^{2(n)}I)^{-1}(z^{(s)}-\mu^{(n)}))-\log(N)
      \tag{13}
      \end{equation}
   $$
      Finally, mutual information can be expressed as:
   $$
      \begin{equation}
      I_{q}=\frac{1}{N} \sum_{n=1}^{N}\frac{1}{S} \sum_{s=1}^{S}[-\sum_{j=1}^{k}\log(\sigma^{(n)}_{j})-\frac{1}{2}(z^{(s)}-\mu^{(n)})^{T}(\sigma^{2(n)}I)^{-1}(z^{(s)}-\mu^{(n)})+\log(\sum_{n=1}^{N} \frac{1}{|\sigma^{2(n)}I|^{\frac{1}{2}}}\exp(-\frac{1}{2}(z^{(s)}-\mu^{(n)})^{T}(\sigma^{2(n)}I)^{-1}(z^{(s)}-\mu^{(n)}))]+\log(N)
      \tag{14}
      \end{equation}
   $$
      We use this representation in our experiments. 

   When KL divergence and Mutual Information are near to zero, It means the latent variable $z$ is not contained any valuable information from the input sentences. And it cause the model to degenerate as conventional language modeling. The model that encodes useful information from the input sentences into the latent variable $z$ will have non zero kl divergence, mutual information and a relatively small cross entropy. [2]

   

    In addition, the extrinsic evaluation is extended version of intrinsic evaluation especially representation learning. In imbalanced data set situation, data augmentation is technique to overcome the imbalanced problem. In this situation, if trained model has good representative latent variables, it will generate diverse and plausible sentences. And we can use this generated sentences for data augmentation. 

   For extrinsic evaluation, we will use news data which are related to scorching heat damage in Korea. This data has two labels, About 130 thousand news are collected and labeled 8,048 news as 0,1. 0 is non related to disaster(such as advertisement) and 1 is related to disaster. We choose this data because of imbalanced situation. The 1,301 articles are related to disaster(minority class), and 6,747 articles are not related to disaster(majority class). Our model train just for minority class, and generate minority class. In the end, we use augmented data for classification.  The procedure of extrinsic evaluation is represented in fig (8).  We will show our result compared to 2 trained models, one is not augmented data and second is not augmented data but sample data for each class evenly. Table (4) shows the classifiers for extrinsic evaluation. BERT [13] is used for word representation. 

   ![data_augmentation](E:\paper_mine\data_augmentation.png)

   <left> fig.(8). Procedure of extrinsic evaluation. </left>

   | Model                 | Parameters             | Value |
   | --------------------- | ---------------------- | ----- |
   | Transformer based VAE | model  dim             | 256   |
   |                       | hidden  dim            | 1024  |
   |                       | heads                  | 4     |
   |                       | layers                 | 2     |
   |                       | drop  out              | 0.1   |
   |                       | activation  function   | GeLU  |
   | LSTM based VAE        | model  dim             | 256   |
   |                       | layers                 | 2     |
   |                       | encoder  bidirectional | TRUE  |
   |                       | drop  out              | 0.1   |
   | LSTM                  | model  dim             | 256   |
   |                       | layers                 | 2     |
   |                       | drop  out              | 0.1   |

   <left> Table. (1). Parameter settings for each model</left>

   

4. Settings

   In Penn Tree Bank, we build vocabulary which appears more than 3 times. We fix sequence length as 32 (include $<BOS>$ token or $<CLS>$ token and $<EOS>$ token), when the sentence is larger than 32 it is truncated and less than 32 it is padded. And the dimension of latent variable is 128.

   The encoder/decoder of Transformer model is composed of a stack of 2 identical encoder/decoder layers respectively.  The model dimension is 256 and the hidden dimension is 1024. And we use 4 multi head attention. And we use GeLU[18] activation function in each layers. 

   The encoder of LSTM model is bidirectional and decoder is unidirectional. We use hidden dimension as 256, and 2 hidden layers for both the encoder and decoder. 

   We use hidden dimension of LSTM language modeling as 256 and 2 hidden layers.

    The model is optimized by AdamW optimizer [19] with cosine annealing [20] for learning rate. We use initial learning rate as 1e-4, final learning rate as 1e-7 which restarts with every 10 epochs. We set the momentum parameters $\beta_1$ = 0.9, $\beta_2$=0.999 and weight decay as 0.1. Batch size is 32, and we trained our model in 100 epochs, we pick the best model in validation data set.

   To prevent kl vanishing,  we use linear annealing[2] and cyclical annealing[8].

   In news data for extrinsic evaluation, we use pretrained BERT and tokenizer in huggingface library. For training  Transformer based variational autoencoder, we fix sequence length as 2,048 (include $<BOS>$ token or $<CLS>$ token and $<EOS>$ token), when the sentence is larger than 2,048 it is truncated and less than 2,048 it is padded. And parameters of Transformer is same as in Penn Tree Bank data set  

    After train Transformer based variational autoencoder, we generate 5,600 sentences same as difference between minority class and majority class. For text generation, we use greedy decoding and remove duplicated sentences. 

    In order to make augmented train, validation data set, we split mixed data set which is sum of train data set, validation data set and generated sentences. The BERT classifier is optimized by AdamW optimizer with cosine annealing for learning rate. We use initial learning rate as 1e-6, final learning rate as 1e-8 which restarts with every 10 epochs. We set the momentum parameters $\beta_1$ = 0.9, $\beta_2$=0.999 and weight decay as 1e-2. Batch size is 16 and we pick the best model in validation data set. And because of limit of BERT in Korean version, we truncate and pad sentences with length 512. To compensate for this, we add length of sentences for input of BERT classifier.

## Result

### Intrinsic evaluation

As described in previous section, we compared our model with LSTM variational auto encoder and LSTM language modeling in intrinsic evaluation.

| MODEL                  | TRAIN   |        |        | VAL     |         |        | TEST        |             |           |            |            |
| ---------------------- | ------- | ------ | ------ | ------- | ------- | ------ | ----------- | ----------- | --------- | ---------- | ---------- |
|                        | Recon   | KL     | MI     | Recon   | KL      | MI     | Recon       | KL          | MI        | dist1      | dist2      |
| VAE+LSTM               | 79.9508 | 0.0027 | 0.0025 | 90.7235 | 0.0026  | 0.0026 | 90.1106     | 0.0027      | 0.0027    | 0.5718     | 0.8445     |
| VAE+LSTM+Linear        | 79.899  | 0.0044 | 0.0042 | 90.6752 | 0.0044  | 0.0045 | 89.9962     | 0.0043      | 0.0039    | 0.5884     | 0.8633     |
| VAE+LSTM+Cycle         | 77.0087 | 7.6525 | 3.413  | 85.7997 | 7.6261  | 3.4097 | 85.1519     | 7.656       | 3.4132    | 0.6148     | **0.8981** |
| VAE+Transformer        | 77.553  | 0.0893 | 0.0886 | 92.3667 | 0.0864  | 0.0766 | 91.6773     | 0.0823      | 0.0777    | 0.615      | 0.8629     |
| VAE+Transformer+Linear | 77.5935 | 0.4727 | 0.4171 | 91.8769 | 0.436   | 0.377  | 91.1526     | 0.4134      | 0.3629    | 0.6171     | 0.8647     |
| VAE+Transformer+Cycle  | 60.164  | 38.186 | 3.4643 | 82.6391 | 22.2938 | 3.4643 | **81.8163** | **22.2688** | **3.464** | 0.6227     | 0.8595     |
| LSTM                   | 80.3065 | -      | -      | 93.0466 | -       | -      | 92.5699     | -           | -         | **0.6871** | 0.8972     |

<left> Table. (2). Langauge modeling evaluation in terms of cross entropy, kl divergence, mutual information and dist. Linear means kl annealing with linear function and Cycle means kl annealing with cyclic function. Bolds represents the best in test set</left>

Table. (2) shows Transformer based variational auto encoder is better than LSTM based variational auto encoder and LSTM language modeling in terms of reconstruction and representation learning. Note that variational auto encoder with Transformer is better than variational auto encoder with LSTM for every kl annealing strategy where cyclic kl annealing strategy model is the best. 

 But in divergence, there are little difference between LSTM based variational auto encoder and Transformer based auto encoder.

 We show the learning curves for Transformer based VAE and LSTM based VAE in train data set as shown in fig.(9). In cross entropy term, Transformer based VAE is much lower than LSTM based. And also converge faster than LSTM based. In every kl annealing strategy, Transformer based VAE is larger than LSTM based VAE in KL divergence. 

![larningcurve병합](E:\paper_mine\learningcurve병합.png)

<left> fig. (9). Learning Curves in train data set. </left>

 We generate sentences from latent variable $z$ sampled from prior distribution $N(0,I)$. We use beam search in decoding with beam width 5. The results is shown in Table. (3). The results show diverse sentences from prior distribution.

| generated                                                    |
| ------------------------------------------------------------ |
| the new york stock exchange  market closed at $ n a share down n cents |
| in new york stock exchange  composite trading <unk> closed at $ n a share down n n cents |
| but he said he does n't expect  a <unk> n n of the <unk>     |
| in the meantime the  <unk> of the <unk> group said it will sell a $ n million or n  cents a share to  n a share |
| a spokesman for the first nine  months of the company said the sale of a $ n million or n cents a share |
| sales for the quarter were up  n n to $ n billion from $ n billion |
| a lot of people in the past  few weeks says <unk> a <unk> <unk> |

<left> Table. (3). Sentence generation sampled from N(0,I) in Transformer VAE with cyclic kl annealing model. </left>

### Extrinsic evaluation<!--(실험 진행 중)-->

We compare classifier 1,2,3 as discussed earlier. The classifiers are evaluated by cross entropy loss, precision, recall, f1 score and accuracy. We use same test data set for evaluation. The result is shown in Table. (5).

| classifier  | Data set   | Sampling           | Train data set | Val data set | Test data set |
| ----------- | ---------- | ------------------ | -------------- | ------------ | ------------- |
| classifier1 | Imbalanced | Random  Sampling   | Imbalanced     | Imbalanced   | Imbalanced    |
| classifer2  | Imbalanced | Weighted  Sampling | balanced       | balanced     | Imbalanced    |
| classifier3 | Augmented  | Random  Sampling   | balanced       | balanced     | Imbalanced    |



<left> Table. (4). Classifers for extrinsic evaluation</left>

|              |      | precision | recall | f1     | support |
| ------------ | ---- | --------- | ------ | ------ | ------- |
| classifier 1 | 0    | 0.9185    | 0.9501 | 0.9340 | 842     |
|              | 1    | 0.6744    | 0.5506 | 0.6063 | 158     |
| classifier 2 | 0    | 0.9766    | 0.6948 | 0.8119 | 842     |
|              | 1    | 0.3591    | 0.9114 | 0.5152 | 158     |
| classifier 3 | 0    | 0.9309    | 0.9442 | 0.9375 | 842     |
|              | 1    | 0.6781    | 0.6266 | 0.6513 | 158     |

|              | accuracy | cross entropy |
| ------------ | -------- | ------------- |
| classifier 1 | 0.8870   | 0.03183       |
| classifier 2 | 0.7290   | 0.07741       |
| classifier 3 | 0.8940   | 0.03549       |

<left> Table. (5). Results for extrinsic evaluation</left>



# Conclusion

 This paper introduce the use of Transformer based variational auto encoder for sentences. We compare the model with RNN based variational auto encoder, and our model trains faster than RNN based model. And also, In reconstruction, representation learning, our model is better than RNN based model. 

<!--classifier에 대한 부분 추가 예정-->

 Furthermore, we use our model in imbalanced dataset. The results show that augmented dataset have the best classification results. It means that the latent variable of our model learn the holistic feature of input sentences well.

<!---->

 We hope in future work  to combine our model with the state of art pretrained word embeddings like GPT, BERT, BART.  And use the model in imbalanced dataset to generate sentences.



# Reference

[1] Diederik P Kingma and Max Welling. 2013. Autoencoding variational bayes. ICLR.
[2] Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. 2015. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349.
[3] Weidi Xu, Haoze Sun, Chao Deng, and Ying Tan. 2017. Variational autoencoder for semi-supervised text classification. In AAAI.
[4] Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. Neural computation. 
[5] Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. 2019. Lagging inference networks and posterior collapse in variational autoencoders. ICLR. 
[6]Shengjia Zhao, Jiaming Song, and Stefano Ermon. 2019. InfoVAE: Information maximizing variational autoencoders. AAAI.
[7] M. Arjovsky, S. Chintala, and L. Bottou. 2017. Wasserstein gan. arXiv preprint arXiv:1701.07875.
[8] Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence Carin. 2019. Cyclical annealing schedule: A simple approach to mitigating kl vanishing. arXiv preprint arXiv:1903.10145.
[9] Liu, D., and Liu, G. 2019. A transformer-based variational autoencoder for sentence generation. In 2019 International Joint Conference on Neural Networks (IJCNN), 1–7. IEEE.
[10] Yu Duan, Jiaxin Pei, Canwen Xu, and Chenliang Li. 2019. Pre-train and plug-in: Flexible conditional text generation with variational auto-encoders. arXiv preprint arXiv:1911.03882.
[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS.
[12] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. 2014. Empirical evaluation
of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555.
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
[14] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openaiassets/research-covers/languageunsupervised/language understanding paper. pdf
[15] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics. 
[16] Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2122–2132
[17] M. D. Hoffman and M. Johnson. 2016. Elbo surgery: yet another way to carve up the variational evidence lower bound. In NIPS Workshop on Advances in Approximate Bayesian Inference. 
[18] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415.
[19] Loshchilov, I. and Hutter, F. 2019. Decoupled Weight Decay Regularization. In International Conference on Learning Representations. arXiv: 1711.05101. 
[20] Ilya Loshchilov and Frank Hutter. 2017. SGDR:Stochastic Gradient Descent with Warm Restarts. In Proceedings of the Internal Conference on Learning Representations 2017. 