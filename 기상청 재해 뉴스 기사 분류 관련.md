# 기상청 재해 뉴스 기사 분류 관련

## 피해기사 인지 아닌 지를 분류 

피해 기사의 피해 부문은 보건, 축산업, 수산양식, 농업, 산업, 교통, 전력, 기타 피해 등 총 8가지로 구성

labeling한 기사 8,048개 중 피해 관련 기사는 1,301개에 불과 (16%)

labeling을 더 할 시간이 부족해서 피해/비피해로 부문을 축소해서 분류 진행 (그래도 매우 unbalance)



## 1.0 전처리

1. [\n]+은 빈칸으로 치환
2. 괄호 안에 들어 간 것과 괄호 들은 제거
3. 숫자는 # 으로 치환
4. [/사진=연합뉴스],기자,배포금지,무단배포,@, 사진 등이 들어가 있는 문장의 경우 아예 삭제

![img](file:///C:/Users/IDSL/Desktop/%EC%9D%B4%EB%AF%B8%EC%A7%80%206.png?lastModify=1603837758?lastModify=1603837758?lastModify=1603837758)

## 1.1 OKT로 tokenize 실시 후 W2V로 word representation  

window size = 5, dim = 128, vocab size : 13468

**(1) train data : under sampling을 진행해서 학습 진행 ** - 실험 1

**(2) train data : 그냥 진행, 다만 cross entropy loss에 weight를 [0,1] class에 대해서 해당 class에 속하는 data 개수의 역수로 줌**

## 1.2 LSTM 

bi directional = True, num_layers = 1

classifier는 간단한 neural net 활용 (drop out 0.3)

optimizer로는 AdamW를 활용 (learning rate 1e-4, weight decay : 0.3)

Epoch : 100

### 결과

< 실험 1 >

![이미지 9](C:\Users\IDSL\Desktop\이미지 9.png)

**recall에 좀 더 치중해야 한다는 의견을 반영해서 recall이 92%에 달성해서 마무리 완료**

< 실험 2 >

cross entropy loss에 weight를 부여했다. class 0에는 0.1 class 1에는 0.9 하지만 f1 score는 더 좋지 않았음.

![이미지 11](C:\Users\IDSL\Desktop\이미지 11.png)

## 2. BERT

hugging face에 있는 multilingual cased를 활용 (vocab size : 8002에 불과)

embedding dim 768

![BERT 논문정리](https://mino-park7.github.io/images/2019/02/bert-input-representation.png)

## 2.1 전처리 작업

​	문장의 시작과 끝에 [CLS]와 [SEP]을 붙여준다

## 2.2 Sampling 작업

WeightedRandomSampling 작업 진행

## 2.3 학습

Fine Tunning : epoch 20으로 

###결과

Train

![이미지 15](C:\Users\IDSL\Desktop\이미지 15.png)

Test

![이미지 14](C:\Users\IDSL\Desktop\이미지 14.png)